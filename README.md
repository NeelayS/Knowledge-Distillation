# Knowledge-Distillation

Implementation pf the paper Distilling the Knowledge in a Neural Network[https://arxiv.org/abs/1503.02531].

**Results on MNIST**

Test set size = 10000

|Model | Accuracy|
|--- |---|
|Large model | 0.9847|
|--- |---|
|Distilled model | 0.9810|
